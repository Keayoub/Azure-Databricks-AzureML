# Terraform: Unity Catalog Configuration

This directory contains Terraform configurations for deploying and managing Databricks Unity Catalog components.

## Architecture

```
terraform/
├── metastore/              # Account-level metastore (one-time, per region)
└── environments/           # Workspace-level UC components (catalogs, schemas, volumes)
```

## Two-Layer Terraform Deployment

### Layer 1: Metastore (`terraform/metastore/`)

**Purpose:** Create the Unity Catalog metastore (account-level)

**When to run:**
- Automatically via `azd provision` → `postprovision.ps1`
- Or manually: `cd terraform/metastore && terraform apply`

**What it does:**
- Creates one metastore per region (e.g., `metastore-dbxaml` in Canada East)
- Configures managed identity access via Access Connector
- Assigns the workspace to the metastore
- Sets default namespace to `main` catalog

**Key variables** (auto-generated by postprovision):
```hcl
databricks_account_id       # From DATABRICKS_ACCOUNT_ID env var
databricks_workspace_id     # Extracted from workspace URL
databricks_workspace_host   # From Bicep deployment
project_name                # From Bicep parameters
databricks_region           # From workspace location
```

**Idempotency:** ✅ Safe to run multiple times. If metastore exists, it skips creation.

### Layer 2: UC Components (`terraform/environments/`)

**Purpose:** Create catalogs, schemas, and volumes

**When to run:**
- Automatically via `azd deploy` → `postdeploy.ps1`
- Or manually: `cd terraform/environments && terraform apply`

**What it does:**
- Creates catalogs (e.g., `dev_lob_team_1`, `prod_lob_team_1`)
- Creates schemas (bronze, silver, gold medallion architecture)
- Creates external volumes and storage credentials
- Assigns ownership and permissions

**Key variables** (auto-generated by postdeploy):
```hcl
environment_name            # dev, staging, prod
databricks_workspace_host   # From Bicep deployment
azure_region                # From workspace location
```

---

## Quick Start

### 1. Automatic Deployment (via azd)

```bash
# Phase 1: Deploy infrastructure + metastore
azd provision

# Phase 2: Deploy UC components
azd deploy
```

### 2. Manual Step-by-Step Deployment

```bash
# Step 1: Create metastore
cd terraform/metastore
terraform init
terraform plan -out=tfplan
terraform apply tfplan

# Step 2: Create UC components
cd ../environments
terraform init
terraform plan -out=tfplan
terraform apply tfplan
```

### 3. Manual with Custom Variables

```bash
# Create custom tfvars file
cat > custom.tfvars << EOF
environment_name            = "prod"
databricks_workspace_host   = "https://adb-...azuredatabricks.net"
azure_region                = "canadaeast"
EOF

# Plan and apply with custom variables
cd terraform/environments
terraform plan -var-file=custom.tfvars -out=tfplan
terraform apply tfplan
```

---

## File Structure

### Metastore Layer

```
metastore/
├── main.tf                 # Metastore creation and assignment
├── providers.tf            # Provider configuration (account-level)
├── variables.tf            # Input variables
├── outputs.tf              # Output values (metastore ID, etc.)
└── (terraform.tfvars)      # Generated by postprovision.ps1
```

### Environments Layer

```
environments/
├── main.tf                 # UC components configuration
├── providers.tf            # Provider configuration (workspace-level)
├── variables.tf            # Input variables
├── outputs.tf              # Output values
└── (terraform.tfvars)      # Generated by postdeploy.ps1
```

---

## Modules

### adb-uc-catalogs

Creates Unity Catalog structures: catalogs, schemas, and ownership.

```hcl
module "uc-catalogs" {
  source = "./modules/adb-uc-catalogs"
  
  metastore_id = databricks_metastore.primary.id
  
  catalogs = {
    dev_team_1 = {
      name    = "dev_team_1"
      comment = "Development environment - Team 1"
      schemas = {
        bronze  = { name = "bronze", comment = "Raw data" }
        silver  = { name = "silver", comment = "Cleaned data" }
        gold    = { name = "gold", comment = "Analytics-ready" }
      }
    }
  }
}
```

See [modules/adb-uc-catalogs/README.md](./modules/adb-uc-catalogs/README.md)

### adb-uc-volumes

Creates external volumes for Databricks file storage.

```hcl
module "uc-volumes" {
  source = "./modules/adb-uc-volumes"
  
  catalogs = {
    dev_team_1 = {
      catalog_name  = "dev_team_1"
      schema_name   = "bronze"
      name          = "raw-data"
      external_location_id = databricks_external_location.main.id
    }
  }
}
```

See [modules/adb-uc-volumes/README.md](./modules/adb-uc-volumes/README.md)

---

## Environment Variables

The deployment scripts automatically set these variables. You only need to set `DATABRICKS_ACCOUNT_ID`:

```powershell
# Required: Databricks Account ID
$env:DATABRICKS_ACCOUNT_ID = "b90dde1c-048c-4a28-b7d1-6c7c4df24b90"

# Optional: Set Azure subscription (if not using az login context)
$env:ARM_SUBSCRIPTION_ID = "c7b690b3-d9ad-4ed0-9942-4e7a36d0c187"

# Optional: Set Terraform debug logging
$env:TF_LOG = "DEBUG"
$env:TF_LOG_PATH = "./terraform.log"
```

---

## Troubleshooting

### Missing Variables

**Error:** `Error: Missing required argument`

**Solution:** Ensure `terraform.tfvars` is generated:
```bash
# Metastore layer
cd terraform/metastore
pwsh ../../infra/scripts/postprovision.ps1

# UC components layer
cd terraform/environments
pwsh ../../infra/scripts/postdeploy.ps1
```

### Metastore Not Assigned

**Error:** `Error: cannot assign metastore: workspace already has a metastore`

**Solution:** The workspace is already assigned. Run `terraform import` to adopt it:
```bash
cd terraform/metastore
terraform import databricks_metastore_assignment.workspace <workspace-id>
terraform apply
```

### Provider Configuration Issues

**Error:** `Error: Invalid Databricks Account configuration`

**Solution:** Ensure you set the account ID before running:
```bash
$env:DATABRICKS_ACCOUNT_ID = "your-account-id"
terraform plan
```

### State File Issues

**Error:** `Error: Failed to read state file`

**Solution:** Reinitialize Terraform:
```bash
cd terraform/metastore
rm -r .terraform .terraform.lock.hcl
terraform init
terraform plan
```

---

## Best Practices

1. **Always review plans before applying**
   ```bash
   terraform plan -out=tfplan  # Review first
   terraform apply tfplan       # Then apply
   ```

2. **Use consistent variable files**
   - Keep environment-specific values in separate tfvars files
   - Example: `dev.tfvars`, `staging.tfvars`, `prod.tfvars`

3. **Version your modules**
   - Pin module versions in `terraform.tf`
   - Use semantic versioning for module updates

4. **Tag all resources**
   - Apply consistent tags for billing and organization
   - Example: `Environment`, `CostCenter`, `Owner`

5. **Validate before committing**
   ```bash
   terraform fmt -recursive           # Format code
   terraform validate                 # Validate syntax
   terraform plan                     # Check changes
   ```

---

## State Management

State files are stored **locally** in each directory:

```
terraform/
├── metastore/.terraform/terraform.tfstate
└── environments/.terraform/terraform.tfstate
```

These are excluded from Git via `.gitignore`. To safely share state:
- Use Azure Storage backend (optional setup required)
- Share plan files (`tfplan`) instead of raw state
- Keep state files out of version control

---

## Deployment Workflow

### First Deployment (Fresh Infrastructure)

```
1. azd provision
   └─ Deploys Bicep infrastructure
   └─ Runs postprovision hook
      └─ Creates metastore
      └─ Assigns workspace to metastore

2. azd deploy
   └─ Runs postdeploy hook
   └─ Deploys UC catalogs and schemas
```

### Subsequent Deployments

```
1. azd provision (updates Bicep only)
   └─ Updates Azure resources as needed

2. azd deploy
   └─ Updates Terraform configurations
   └─ Adds/modifies catalogs and schemas
```

### Manual Updates (Advanced)

```bash
cd terraform/environments

# Review what will change
terraform plan

# Apply specific resource
terraform apply -target=databricks_catalog.team_1

# Destroy a catalog (careful!)
terraform destroy -target=databricks_catalog.team_1
```

---

## Documentation

- [Deployment Process (Full Guide)](../docs/DEPLOYMENT-PROCESS.md)
- [Terraform Quick Reference](./docs/TERRAFORM-QUICK-REFERENCE.md)
- [Terraform Quick Start](./docs/TERRAFORM-QUICK-START.md)

---

## Support

For issues or questions:

1. Check [Terraform Logs](./terraform.log)
2. Review [Troubleshooting](#troubleshooting) section
3. Run validation: `terraform validate && terraform plan`
4. Check Databricks workspace admin console for metastore details
5. Review [Deployment Process docs](../docs/DEPLOYMENT-PROCESS.md)
